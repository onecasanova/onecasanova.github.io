---
layout: post
title: "Optimizing AI/ML Model for Breast Cancer Malignancy Detection in Lower Income Countries"
date: 2025-04-17 -0500
permalink: /projects/AIML
categories: projects
---

<!-- inputing an image-->
<!-- with centering of image -->
<figure style="text-align: center;">
    <img src="{{ site.baseurl }}/assets/AIML/breast_cytology.jpg" alt="Image" width="650" style="display: block; margin: 0 auto;">
    <figcaption style="font-style: italic; color: #aaa;">Image from https://acsjournals.onlinelibrary.wiley.com/doi/full/10.1002/cncr.20735 </figcaption>
</figure>

## What
This project involved two main components. The first being to develop an AI/ML model that can detect, with >80% accuracy, if a breast cancer cytology slide (as seen above) has breat cancer or not, or if it is in the "other" category which is indeterminate. The second aspect of this project was to see how much better we could make this model such that it still performed well, or had good performance, under adverse conditions. These adverse conditions include blurry, too bright, or too dim images.

## How 

We were given a dataset of ~1,100 real life digital cytology slide images to work with. With this we used 60% of the images to train a small pre-trained convolutional neural network CNN, namely EfficientNet B0 which is used for compuationally cheap image classification.  

<figure style="text-align: center;">
    <img src="{{ site.baseurl }}/assets/AIML/efficient-net.jpg" alt="Image" width="650" style="display: block; margin: 0 auto;">
    <figcaption style="font-style: italic; color: #aaa;">Comparison of Efficient Net with different neural nets on Imagenet </figcaption>
</figure>

After fine tuning this model to accurately evaulate the images in our specific dataset, we then augmented our data (blurry, bright, dim) and evaluated our model on the new augmented images. We then used this as a baseline for how a reasonable model performs with good and "bad" images. Next, we retrained the model on the augmented images, multiple times, and test it again with the augmented images, or "bad" data, to see how it performs. 

## Results
The model performed exceptionally better than the original model under worse conditions. All in all, a more robust model was developed as a result. When the augmented images, or bad quality images, were passed through the new model the performance was much better. See the figure below.

<figure style="text-align: center;">
    <img src="{{ site.baseurl }}/assets/AIML/combined_accuracy.jpg" alt="Image" width="800" height="200" style="display: block; margin: 0 auto;">
    <figcaption style="font-style: italic; color: #aaa;">Accuracy comparison for all three image augmentation catgories between basline trained model and robust model. </figcaption>
</figure>

For a quantifiable result, the new model performed 84% better under the adverse conditions than the original model, mainly on the blurry images.


## Challenges

One of the main challenges to this project was retraining the baseline model with the larger dataset with augmented images. Originally we had trained it with many more augmented images than the original images so the new model had overfit to the lower quality images. So when we evaluated the new model, the accuracy dropped for the regular images, and it did better on lower quality images. That's not what we wanted. A solution to this was to make sure the set used for training had an even split half/half of good images and augmented images so that it didn't overfit. In addition, we had to train this model multiple times over the different augmentation styles (blur, bright, dim) instead of all at once. And each time we retrained the split had to be even. This strategy ended up retaining high accuracy for the regular images and improving accuracy for augmented images.


